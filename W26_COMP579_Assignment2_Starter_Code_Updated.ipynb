{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmPE6JJQ9ov2"
   },
   "source": [
    "# Winter 2026 COMP 579 Assignment 2 Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlMwZ2ikhAu8"
   },
   "source": [
    "In this assignment, you will experiment with different algorithms to evaluate a given fixed policy.\n",
    "\n",
    "Following the instructions in the questions document, fill in the missing code indicated by \"TODO\" marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Dq5-1rrx9b35"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "from typing import Dict, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KpyVp6adiio"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTTaOk8EVNsT"
   },
   "source": [
    "Do NOT change the RANDOM_SEED value set in the next code cell. This ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ue1glys3VOS8"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk0gixgthzHJ"
   },
   "source": [
    "We will start by initialising our environment:\n",
    "\n",
    "> Before you proceed, we recommend that you check the Gymnasium Documentation: https://gymnasium.farama.org/ and play around with the Frozen Lake environment https://gymnasium.farama.org/environments/toy_text/frozen_lake/ to get more comfortable with Gym, as you will be frequently using it in RL and the next assignment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x6AJ0nKUkGax"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "env.reset(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F4OlET8NkIHj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 16\n",
      "Actions: 4\n"
     ]
    }
   ],
   "source": [
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "states = list(range(number_of_states))\n",
    "actions = list(range(number_of_actions))\n",
    "\n",
    "print(\"States:\", number_of_states)\n",
    "print(\"Actions:\", number_of_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Uai9jDMjxPw"
   },
   "source": [
    "In Frozen Lake, the action space is defined as:\n",
    "\n",
    "```\n",
    "0: Move left\n",
    "1: Move down\n",
    "2: Move right\n",
    "3: Move up\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZEdFb2BR37BT"
   },
   "outputs": [],
   "source": [
    "P = env.unwrapped.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdqzJmL5nohD"
   },
   "source": [
    "`P` gives the transition information for the Markov Decision Process (MDP). In Gym’s tabular environments (like FrozenLake), `P` is a dictionary of dictionaries of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "73kP7jUokKwx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhWe1hpZijq7"
   },
   "source": [
    "\n",
    "\n",
    "`P[state][action]` is a list of possible outcomes when taking \"action\" in a particular \"state\", because taking an action in a stochastic environment can lead to multiple next states with different probabilities.\n",
    "Each element in this list is a tuple of the form:\n",
    "`(transition probability, next state, reward, is the next next a terminal state?)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Bxz0TkCUkMfa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 4, 0.0, False),\n",
       " (0.3333333333333333, 1, 0.0, False)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP4sCSAFjnzn"
   },
   "source": [
    "So `P[0][1]` shows all possible outcomes of taking action 1 in state 0, including how likely each outcome is, the resulting state, the reward, and whether the episode ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn0uMZK7kEvG"
   },
   "source": [
    "We can visualize our grid by checking the \"desc\" attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9bsmIBy0kN3_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'S', b'F', b'F', b'F'],\n",
       "       [b'F', b'H', b'F', b'H'],\n",
       "       [b'F', b'F', b'F', b'H'],\n",
       "       [b'H', b'F', b'F', b'G']], dtype='|S1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vg6sn5xmkRu5"
   },
   "source": [
    "```\n",
    "“S” for Start tile\n",
    "“G” for Goal tile\n",
    "“F” for frozen tile\n",
    "“H” for a tile with a hole\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fPCl8prOHSF"
   },
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JulruelsOGIU"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "CONVERGENCE_THRESHOLD = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02egZQBJNXOX"
   },
   "source": [
    "### Implementing the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GuMfQ3C1c1h2"
   },
   "outputs": [],
   "source": [
    "def get_terminal_states(P):\n",
    "    terminal_states = set()\n",
    "\n",
    "    for s in P:\n",
    "        # if s == 15:\n",
    "        #     break\n",
    "        actions = P[s]\n",
    "        for a in actions:\n",
    "            next_states = P[s][a]\n",
    "            \n",
    "            all_done_flags_true = True\n",
    "            for ns in next_states:\n",
    "                done = ns[3]\n",
    "                if not done:\n",
    "                    all_done_flags_true = False\n",
    "                    \n",
    "            if all_done_flags_true:\n",
    "                terminal_states.add(s)\n",
    "\n",
    "    return terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pf5OASkfkQEp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5, 7, 11, 12, 15}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terminal_states = get_terminal_states(P)\n",
    "terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XRZqVT9mkRKh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "desc = env.unwrapped.desc\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ut7GiinvCMz3"
   },
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rE1ACq3PCu0S"
   },
   "outputs": [],
   "source": [
    "def get_valid_actions(env, s):\n",
    "  \"\"\"\n",
    "  Given an env and a state, return a list of valid actions at this state.\n",
    "  An action is valid if it leads to a different state with non-zero probability\n",
    "  \"\"\"\n",
    "  valid_actions = []\n",
    "  for a in range(env.action_space.n):\n",
    "      for (prob, next_s, _, _) in P[s][a]:\n",
    "          if prob > 0 and next_s != s:\n",
    "              valid_actions.append(a)\n",
    "              break\n",
    "  return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "g-ZBbIOI5hYr"
   },
   "outputs": [],
   "source": [
    "def right_favoring_policy(env, s):\n",
    "    pi = np.zeros(number_of_actions)\n",
    "\n",
    "    # skip terminal states\n",
    "    if s in terminal_states:\n",
    "        return pi\n",
    "\n",
    "    valid_actions = get_valid_actions(env, s)\n",
    "\n",
    "    if len(valid_actions) == 1:\n",
    "        pi[valid_actions[0]] = 1\n",
    "        return pi\n",
    "\n",
    "    # If “Move right” is a valid action\n",
    "    if RIGHT in valid_actions:\n",
    "        pi[RIGHT] = 0.7\n",
    "        other_prob = (1 - 0.7) / (len(valid_actions) - 1)\n",
    "    else:\n",
    "        other_prob = 1 / len(valid_actions)\n",
    "\n",
    "    # set the probability for the rest of the actions\n",
    "    for a in valid_actions:\n",
    "        if a == RIGHT:\n",
    "            continue\n",
    "        pi[a] = other_prob\n",
    "\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jiKktgRNdKs"
   },
   "source": [
    "### Deriving $v_\\pi$ with Matrix Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_AOglHFhN5jL"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation_analytical(env, policy, gamma):\n",
    "\n",
    "    r_pi = np.zeros(number_of_states)\n",
    "    P_pi = np.zeros((number_of_states, number_of_states))\n",
    "\n",
    "    # build r_pi and P_pi\n",
    "    for s in states:\n",
    "        pi_s = policy(env, s)  # (0.7, 0.1, 0.1, 0.1)\n",
    "\n",
    "        for a in get_valid_actions(env, s):\n",
    "            pi_a = pi_s[a]  # 0.7\n",
    "\n",
    "            for ns in P[s][a]:  # (prob, next_s, reward, done)\n",
    "                prob = ns[0]\n",
    "                next_s = ns[1]\n",
    "                r = ns[2]\n",
    "                r_pi[s] += prob * r * pi_a\n",
    "                P_pi[s, next_s] += pi_a * prob\n",
    "\n",
    "    I = np.eye(number_of_states)\n",
    "\n",
    "    V = np.linalg.inv(I - gamma * P_pi) @ r_pi\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "W0CiNH7xkS9g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00917343, 0.00857936, 0.02013823, 0.00394009, 0.01316506,\n",
       "       0.        , 0.04764794, 0.        , 0.0351977 , 0.10546422,\n",
       "       0.15633561, 0.        , 0.        , 0.22254004, 0.49621811,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_analytical = policy_evaluation_analytical(env, right_favoring_policy, gamma=GAMMA)\n",
    "V_analytical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2jjYiSbTprk"
   },
   "source": [
    "### Synchronous DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhcrcboJQaxH"
   },
   "outputs": [],
   "source": [
    "def synchronous_policy_evaluation(P, policy, gamma, tol):\n",
    "  V = np.zeros(number_of_states)\n",
    "  residuals = []\n",
    "\n",
    "  # TODO\n",
    "\n",
    "  return V, residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr4snwO-T0u9"
   },
   "outputs": [],
   "source": [
    "V_sync, residuals = synchronous_policy_evaluation(P, right_favoring_policy, GAMMA, CONVERGENCE_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ02VcQ1kUi7"
   },
   "outputs": [],
   "source": [
    "V_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nj1y_L4CkVtw"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(residuals)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Bellman Residual over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Bellman Residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2VCHesqkL59"
   },
   "source": [
    "### Value Function Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjjefvsVfE8l"
   },
   "outputs": [],
   "source": [
    "def plot_value_heatmap(env, V, title):\n",
    "  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZgXrzelkXQ8"
   },
   "outputs": [],
   "source": [
    "plot_value_heatmap(env, V_sync, title=\"State-Value Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD6ZMu34kOds"
   },
   "source": [
    "### Non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UouEcCBjTe2"
   },
   "outputs": [],
   "source": [
    "def uniform_random_policy(env, s):\n",
    "  pi = np.zeros(number_of_actions) # action-probability vector at state s\n",
    "\n",
    "  #TODO\n",
    "\n",
    "  return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUMenAp5lDNh"
   },
   "outputs": [],
   "source": [
    "def non_stationary_policy_evaluation(P, policy_A, policy_B, gamma, total_steps, switch_interval):\n",
    "  V = np.zeros(number_of_states)\n",
    "  residuals = []\n",
    "  V_norms = []\n",
    "\n",
    "  # TODO\n",
    "\n",
    "  return V, residuals, V_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fn6PJUl8l_ZS"
   },
   "outputs": [],
   "source": [
    "non_stationary_V, non_stationary_residuals, non_stationary_V_norms = non_stationary_policy_evaluation(P, right_favoring_policy, uniform_random_policy, GAMMA, 40, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS_9ZKaKe4EH"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(non_stationary_residuals)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.xlabel(\"Bellman Residual\")\n",
    "plt.title(\"Non-Stationary Bellman Residual over Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUPgQBjSsIyg"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1yn0q8nLiuo"
   },
   "source": [
    "### 2D Random Walk Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G91_f39FE-vU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from math import floor\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VA_ukQZxfL-x"
   },
   "outputs": [],
   "source": [
    "class RandomWalk2DEnv:\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.array([0.5, 0.5])\n",
    "        return self.state.copy()\n",
    "\n",
    "    def sample_action(self):\n",
    "        return np.array([\n",
    "            random.uniform(-0.2, 0.2),\n",
    "            random.uniform(-0.2, 0.2)\n",
    "        ])\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state = self.state + action\n",
    "\n",
    "        # Check termination\n",
    "        done = (\n",
    "            next_state[0] < 0 or next_state[0] > 1 or\n",
    "            next_state[1] < 0 or next_state[1] > 1\n",
    "        )\n",
    "\n",
    "        # Reward function\n",
    "        reward = next_state[0] + next_state[1] if done else 0.0\n",
    "\n",
    "        self.state = next_state\n",
    "        return next_state.copy(), reward, done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz21IXSYVoZe"
   },
   "source": [
    "### Tile coding software (from incompleteideas.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zABM8MmRghmw"
   },
   "outputs": [],
   "source": [
    "basehash = hash\n",
    "\n",
    "class IHT:\n",
    "    \"Structure to handle collisions\"\n",
    "    def __init__(self, sizeval):\n",
    "        self.size = sizeval\n",
    "        self.overfullCount = 0\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            \"Collision table:\"\n",
    "            + \" size:\" + str(self.size)\n",
    "            + \" overfullCount:\" + str(self.overfullCount)\n",
    "            + \" dictionary:\" + str(len(self.dictionary)) + \" items\"\n",
    "        )\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "    def fullp(self):\n",
    "        return len(self.dictionary) >= self.size\n",
    "\n",
    "    def getindex(self, obj, readonly=False):\n",
    "        d = self.dictionary\n",
    "        if obj in d:\n",
    "            return d[obj]\n",
    "        elif readonly:\n",
    "            return None\n",
    "        size = self.size\n",
    "        count = self.count()\n",
    "        if count >= size:\n",
    "            if self.overfullCount == 0:\n",
    "                print(\"IHT full, starting to allow collisions\")\n",
    "            self.overfullCount += 1\n",
    "            return basehash(obj) % self.size\n",
    "        else:\n",
    "            d[obj] = count\n",
    "            return count\n",
    "\n",
    "def hashcoords(coordinates, m, readonly=False):\n",
    "    if type(m) == IHT:\n",
    "        return m.getindex(tuple(coordinates), readonly)\n",
    "    if type(m) == int:\n",
    "        return basehash(tuple(coordinates)) % m\n",
    "    if m is None:\n",
    "        return coordinates\n",
    "\n",
    "\n",
    "def tiles(ihtORsize, numtilings, floats, ints=[], readonly=False):\n",
    "    \"\"\"Returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
    "    qfloats = [floor(f * numtilings) for f in floats]\n",
    "    Tiles = []\n",
    "    for tiling in range(numtilings):\n",
    "        tilingX2 = tiling * 2\n",
    "        coords = [tiling]\n",
    "        b = tiling\n",
    "        for q in qfloats:\n",
    "            coords.append((q + b) // numtilings)\n",
    "            b += tilingX2\n",
    "        coords.extend(ints)\n",
    "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
    "    return Tiles\n",
    "\n",
    "\n",
    "def tileswrap(ihtORsize, numtilings, floats, wrapwidths, ints=[], readonly=False):\n",
    "    \"\"\"Returns num-tilings tile indices with wrapping\"\"\"\n",
    "    qfloats = [floor(f * numtilings) for f in floats]\n",
    "    Tiles = []\n",
    "    for tiling in range(numtilings):\n",
    "        tilingX2 = tiling * 2\n",
    "        coords = [tiling]\n",
    "        b = tiling\n",
    "        for q, width in zip_longest(qfloats, wrapwidths):\n",
    "            c = (q + b % numtilings) // numtilings\n",
    "            coords.append(c % width if width else c)\n",
    "            b += tilingX2\n",
    "        coords.extend(ints)\n",
    "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
    "    return Tiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iS8-V-SuSuhb"
   },
   "source": [
    "### Fixed tiling parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXiaD722FLs1"
   },
   "outputs": [],
   "source": [
    "NUM_TILINGS = 8\n",
    "TILES_PER_DIM = 4          # 4 x 4 per tiling\n",
    "IHT_SIZE = 4096\n",
    "\n",
    "# Global index hash table\n",
    "iht = IHT(IHT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oicozAWegr_s"
   },
   "outputs": [],
   "source": [
    "def tile_coding_indices(state):\n",
    "\n",
    "    x, y = state\n",
    "\n",
    "    return tiles(\n",
    "        ihtORsize=iht,\n",
    "        numtilings=NUM_TILINGS,\n",
    "        floats=[x * TILES_PER_DIM, y * TILES_PER_DIM]\n",
    "    )\n",
    "\n",
    "def tile_coding_feature_vector(state):\n",
    "\n",
    "    phi = np.zeros(IHT_SIZE)\n",
    "    for idx in tile_coding_indices(state):\n",
    "        phi[idx] = 1.0\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8oawVQf1jya"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLgatz-ngxnW",
    "outputId": "d8dda42c-4544-473a-ca76-72a17f76349a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active tiles: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Number of active tiles: 8\n"
     ]
    }
   ],
   "source": [
    "s = (0.5, 0.5)\n",
    "idx = tile_coding_indices(s)\n",
    "\n",
    "print(\"Active tiles:\", idx)\n",
    "print(\"Number of active tiles:\", len(idx))  # should be 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSKZtqbViv0v"
   },
   "source": [
    "### Part b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lQ3r1YZIiSS"
   },
   "source": [
    "#### Every-visit Monte Carlo Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgywZcHQIh-5"
   },
   "outputs": [],
   "source": [
    "class MCAgentTileCoding:\n",
    "    def __init__(self, alpha=0.1):\n",
    "        ## TODO: initialize alpha and weights\n",
    "        pass\n",
    "\n",
    "    def value(self, state):\n",
    "        ## TODO: compute sum of weights for active tiles\n",
    "        pass\n",
    "\n",
    "    def update_episode(self, episode):\n",
    "        ## TODO: compute returns Gt\n",
    "        ## TODO: update weights for each state in the episode\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lihlOCIsIXgb"
   },
   "source": [
    "#### Run MC experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i96UHymIZ-R"
   },
   "outputs": [],
   "source": [
    "def run_mc_experiment(\n",
    "    learning_rates=[0.1, 0.03, 0.01, 0.003, 0.001],\n",
    "    num_episodes=1000,\n",
    "    num_runs=30\n",
    "):\n",
    "    \"\"\"\n",
    "    Run every-visit Monte Carlo prediction experiments using Tile Coding.\n",
    "    Returns mean and std of MSVE for each learning rate.\n",
    "    \"\"\"\n",
    "    mean_msve = {}  # placeholder\n",
    "    std_msve = {}   # placeholder\n",
    "\n",
    "    ## TODO: Implement experiment loop over alphas, runs, episodes\n",
    "\n",
    "    return mean_msve, std_msve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DahTICfHmJH"
   },
   "source": [
    "#### MSVE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPTuytawHlcD"
   },
   "outputs": [],
   "source": [
    "def compute_msve(agent):\n",
    "    \"\"\"\n",
    "    Compute the Mean Squared Value Error (MSVE) over a 21x21 grid\n",
    "    in [0,1] x [0,1] using the true value function V(x, y) = x + y.\n",
    "    \"\"\"\n",
    "    msve = 0.0  # placeholder\n",
    "\n",
    "    ## TODO: Compute MSVE over grid for the given agent\n",
    "\n",
    "    return msve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II_wHrg1IeWM"
   },
   "source": [
    "#### Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Qn9sp7nIdNS"
   },
   "outputs": [],
   "source": [
    "def plot_mc_results(mean_msve, std_msve):\n",
    "    alphas = list(mean_msve.keys())\n",
    "    means = np.array(list(mean_msve.values()))\n",
    "    stds = np.array(list(std_msve.values()))\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.errorbar(alphas, means, yerr=stds, fmt='o-', capsize=5)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Learning rate α\")\n",
    "    plt.ylabel(\"MSVE ± 1 std\")\n",
    "    plt.title(\"Every-Visit MC Prediction with Tile Coding (30 runs)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    learning_rates = [0.1, 0.03, 0.01, 0.003, 0.001]\n",
    "\n",
    "    mean_msve, std_msve = run_mc_experiment(\n",
    "        learning_rates=learning_rates,\n",
    "        num_episodes=1000,\n",
    "        num_runs=30\n",
    "    )\n",
    "\n",
    "    print(\"Mean MSVE ± Std Dev over 30 runs after 1000 episodes:\")\n",
    "    for alpha in learning_rates:\n",
    "        print(f\"α={alpha}: {mean_msve[alpha]:.5f} ± {std_msve[alpha]:.5f}\")\n",
    "\n",
    "    plot_mc_results(mean_msve, std_msve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Na8XzB_eMQ-w"
   },
   "source": [
    "### Part c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uR28ALA5HgG4"
   },
   "source": [
    "#### TD(λ) Agent (Accumulating Traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJrXDYIsHee8"
   },
   "outputs": [],
   "source": [
    "class TDAgentLambda:\n",
    "    def __init__(self, alpha=0.01, gamma=1.0, lam=0.5):\n",
    "        \"\"\"\n",
    "        alpha: learning rate\n",
    "        gamma: discount factor\n",
    "        lam: trace decay parameter\n",
    "        \"\"\"\n",
    "        ## TODO: initialize weights and eligibility traces ##\n",
    "\n",
    "    def value(self, state):\n",
    "        \"\"\"\n",
    "        Return the current value estimate for a given state.\n",
    "        \"\"\"\n",
    "        ## TODO: compute value from active tile indices ##\n",
    "\n",
    "    def reset_traces(self):\n",
    "        \"\"\"\n",
    "        Reset eligibility traces at the start of each episode.\n",
    "        \"\"\"\n",
    "        ## TODO: reset eligibility traces ##\n",
    "\n",
    "    def update(self, state, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Perform a TD(lambda) update for a single step.\n",
    "        \"\"\"\n",
    "        ## TODO: update weights using accumulating traces ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pBreRpxHuAF"
   },
   "source": [
    "#### Run TD(λ) Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVbYGBajHqIy"
   },
   "outputs": [],
   "source": [
    "def run_td_lambda_experiment(\n",
    "    lambdas=[0, 0.25, 0.5, 0.75, 0.9],\n",
    "    alphas=[0.001, 0.003, 0.01, 0.03, 0.1],\n",
    "    num_episodes=1000,\n",
    "    num_runs=30\n",
    "):\n",
    "    results = {}\n",
    "\n",
    "    ## TODO:\n",
    "    # Loop over lambdas and alphas\n",
    "    # For each seed, initialize env with seed and TDAgentLambda\n",
    "    # Run num_episodes, update agent each step\n",
    "    # Compute MSVE over 21x21 grid\n",
    "    # Average MSVE over runs and store in results\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqWkTs3SH7_3"
   },
   "source": [
    "#### Plot 1: MSVE vs α (one curve per λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gTjUCkcHxFq"
   },
   "outputs": [],
   "source": [
    "def plot_msve_vs_alpha(results, lambdas, alphas):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for lam in lambdas:\n",
    "        msves = [results[(lam, alpha)] for alpha in alphas]\n",
    "        plt.plot(alphas, msves, marker='o', label=f\"λ={lam}\")\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Learning rate α\")\n",
    "    plt.ylabel(\"Average MSVE\")\n",
    "    plt.title(\"TD(λ): MSVE vs α\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZoE6esrH5pX"
   },
   "source": [
    "#### Plot 2: MSVE vs λ (one curve per α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDepr00FH0vj"
   },
   "outputs": [],
   "source": [
    "def plot_msve_vs_lambda(results, lambdas, alphas):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for alpha in alphas:\n",
    "        msves = [results[(lam, alpha)] for lam in lambdas]\n",
    "        plt.plot(lambdas, msves, marker='o', label=f\"α={alpha}\")\n",
    "\n",
    "    plt.xlabel(\"λ\")\n",
    "    plt.ylabel(\"Average MSVE\")\n",
    "    plt.title(\"TD(λ): MSVE vs λ\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nQKlyUfIAEa"
   },
   "source": [
    "#### Plot 3: Heatmap (λ × α)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wdv-ISTiH3cq"
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(results):\n",
    "    df = pd.DataFrame(\n",
    "        [(lam, alpha, msve) for (lam, alpha), msve in results.items()],\n",
    "        columns=[\"lambda\", \"alpha\", \"MSVE\"]\n",
    "    )\n",
    "\n",
    "    heatmap_data = df.pivot(index=\"lambda\", columns=\"alpha\", values=\"MSVE\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cmap=\"viridis\")\n",
    "    plt.xlabel(\"Learning rate α\")\n",
    "    plt.ylabel(\"λ\")\n",
    "    plt.title(\"TD(λ) with Accumulating Traces: MSVE\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2FE3TdeID8t"
   },
   "source": [
    "#### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVy-JiiPIFVx"
   },
   "outputs": [],
   "source": [
    "lambdas = [0, 0.25, 0.5, 0.75, 0.9]\n",
    "alphas = [0.001, 0.003, 0.01, 0.03, 0.08]\n",
    "\n",
    "results = run_td_lambda_experiment(\n",
    "    lambdas=lambdas,\n",
    "    alphas=alphas,\n",
    "    num_episodes=1000,\n",
    "    num_runs=30\n",
    ")\n",
    "\n",
    "print(\"Average MSVE over 30 runs after 1000 episodes:\")\n",
    "for (lam, alpha), msve in results.items():\n",
    "    print(f\"λ={lam}, α={alpha}: MSVE={msve:.5f}\")\n",
    "\n",
    "plot_msve_vs_alpha(results, lambdas, alphas)\n",
    "plot_msve_vs_lambda(results, lambdas, alphas)\n",
    "plot_heatmap(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZg2GNxaMHe_"
   },
   "source": [
    "### Part d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8cpxRNsLr5v"
   },
   "source": [
    "#### TD(0) agent with Tile Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FR3coa64E_YC"
   },
   "outputs": [],
   "source": [
    "class TDAgentTileCoding:\n",
    "    def __init__(self, alpha=0.1, gamma=1.0):\n",
    "        ## TODO: initialize alpha, gamma, and weights for tile coding\n",
    "\n",
    "\n",
    "    def value(self, state):\n",
    "        \"\"\"Estimate value of a state using tile coding.\"\"\"\n",
    "        ## TODO: return estimated value using active tile indices\n",
    "\n",
    "    def update(self, state, reward, next_state, done):\n",
    "        \"\"\"TD(0) update for a single transition.\"\"\"\n",
    "        ## TODO: compute TD error and update weights for active tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd1uLRH_Lt03"
   },
   "source": [
    "#### TD(0) agent with 2-layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujR9h5M2LuSy"
   },
   "outputs": [],
   "source": [
    "class NNValueFunction:\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, alpha=1e-3, gamma=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, 1) / np.sqrt(hidden_dim)\n",
    "        self.b2 = np.zeros(1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        self.z1 = np.dot(state, self.W1) + self.b1\n",
    "        self.h1 = np.tanh(self.z1)\n",
    "        value = np.dot(self.h1, self.W2) + self.b2\n",
    "        return value.item()\n",
    "\n",
    "    def td_update(self, state, reward, next_state, done):\n",
    "        ## TODO: compute TD target\n",
    "        ## TODO: compute TD error\n",
    "        ## TODO: backpropagate TD error through network\n",
    "        ## TODO: update weights W1, W2 and biases b1, b2 using alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdI7YJjeL1t4"
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDqgXhhzgGGr"
   },
   "outputs": [],
   "source": [
    "def train_agents(env, num_episodes=2000):\n",
    "\n",
    "    tile_agent = TDAgentTileCoding(alpha=0.1)\n",
    "    nn_agent = NNValueFunction(alpha=1e-3)\n",
    "\n",
    "    tile_values = []\n",
    "    nn_values = []\n",
    "\n",
    "    # ===========================\n",
    "    # TODO: Implement training loop\n",
    "    # ===========================\n",
    "    # For each episode:\n",
    "    # 1. Reset environment\n",
    "    # 2. For Tile Coding agent:\n",
    "    #    - Take steps until termination\n",
    "    #    - Update the tile_agent using TD(0)\n",
    "    # 3. For Neural Network agent:\n",
    "    #    - Reset environment again\n",
    "    #    - Take steps until termination\n",
    "    #    - Update the nn_agent using td_update\n",
    "    # 4. Record the value of the center state [0.5, 0.5] for both agents\n",
    "\n",
    "    return tile_values, nn_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJVjNpMuL8gI"
   },
   "source": [
    "#### Run training and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pBhhiVwzMAa-"
   },
   "outputs": [],
   "source": [
    "env = RandomWalk2DEnv(seed=42)\n",
    "tile_vals, nn_vals = train_agents(env, num_episodes=2000)\n",
    "\n",
    "plt.plot(tile_vals, label=\"Tile Coding\")\n",
    "plt.plot(nn_vals, label=\"Neural Network\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Estimated value at center\")\n",
    "plt.title(\"2D Random Walk: Tile Coding vs Neural Network\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1MHeYcGX54h"
   },
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frdiAfg7X7A8"
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "N = 10\n",
    "r_non_terminal = 0\n",
    "r_terminal = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHivgjtimTnK"
   },
   "outputs": [],
   "source": [
    "def linear_transiton_matrix(N):\n",
    "  \"\"\"\n",
    "  A simple linear transition matrix for N states\n",
    "  At every state (except the terminal one), the agent deterministically moves\n",
    "  to s+1. At the terminal state, the agent stays in the terminal state.\n",
    "  \"\"\"\n",
    "  P = np.zeros((N, N))\n",
    "\n",
    "  for s in range(N-1):\n",
    "        P[s, s+1] = 1.0\n",
    "\n",
    "  P[N-1, N-1] = 1.0\n",
    "\n",
    "  return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lC-Ex8qBmVqY"
   },
   "outputs": [],
   "source": [
    "P = linear_transiton_matrix(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j85d_GUrmg2o",
    "outputId": "f5bc0e85-7ab8-4fe3-d3bd-452ff5ebc9d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMZFBGrioHO8"
   },
   "outputs": [],
   "source": [
    "class LinearChainEnv:\n",
    "  def __init__(self, N, reward_noise=0):\n",
    "    self.N = N\n",
    "    self.terminal = N-1\n",
    "    self.P = linear_transiton_matrix(N)\n",
    "\n",
    "    self.reward_noise = reward_noise\n",
    "\n",
    "    self.reset()\n",
    "\n",
    "  def reset(self):\n",
    "    self.state = 0\n",
    "    return self.state\n",
    "\n",
    "  def step(self):\n",
    "    if self.state == self.terminal:\n",
    "      return self.state, 0.0, True\n",
    "\n",
    "    next_state_prob = self.P[self.state]\n",
    "    next_state = np.random.choice(self.N, p=next_state_prob)\n",
    "\n",
    "    if next_state == self.terminal:\n",
    "      reward = r_terminal + np.random.randn() * self.reward_noise\n",
    "    else:\n",
    "      reward = r_non_terminal + np.random.randn() * self.reward_noise\n",
    "\n",
    "    self.state = next_state\n",
    "    done = self.state == self.terminal\n",
    "    return self.state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxIrMchgmmOP"
   },
   "outputs": [],
   "source": [
    "class TDAgent:\n",
    "  \"\"\"\n",
    "  Update your implementation from Q2 to work with the new linear chain env\n",
    "  Hint: Instead of tile coding, use one-hot encoding for the state (every state is a feature)\n",
    "  \"\"\"\n",
    "  def __init__(self, num_states, alpha=0.01, gamma=GAMMA):\n",
    "    pass\n",
    "\n",
    "  def value(self, state):\n",
    "    pass\n",
    "\n",
    "  def update(self, state, reward, next_state, done):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWyAgUF_n33G"
   },
   "outputs": [],
   "source": [
    "class MCAgent:\n",
    "  \"\"\"\n",
    "  Update your implementation from Q2 to work with the new linear chain env\n",
    "  Hint: Instead of tile coding, use one-hot encoding for the state (every state is a feature)\n",
    "  \"\"\"\n",
    "  def __init__(self, num_states, alpha=0.01, gamma=GAMMA):\n",
    "    pass\n",
    "\n",
    "  def value(self, state):\n",
    "    pass\n",
    "\n",
    "  def update_episode(self, episode):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL3OELWdoTAy"
   },
   "outputs": [],
   "source": [
    "def true_value_linear_chain(N, gamma, r_non_terminal=0, r_terminal=1.0):\n",
    "    V = np.zeros(N)\n",
    "    #TODO: Implement the true value function\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qupT9jIuondX"
   },
   "outputs": [],
   "source": [
    "V_true = true_value_linear_chain(N, GAMMA, r_non_terminal, r_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4e9rMpVo5R4"
   },
   "outputs": [],
   "source": [
    "def compute_msve(agent, V_true):\n",
    "  # TODO: Implement the MSVE function\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9-w3xYYpCVX"
   },
   "outputs": [],
   "source": [
    "def train_td0_and_mc(env, V_true, num_episodes=10000, lr=0.003):\n",
    "  num_states = env.N\n",
    "\n",
    "  td_agent = TD0Agent(num_states, alpha=lr, gamma=GAMMA)\n",
    "  mc_agent = MCAgent(num_states, alpha=lr, gamma=GAMMA)\n",
    "\n",
    "  td_values = []\n",
    "  mc_values = []\n",
    "  td_msve = []\n",
    "  mc_msve = []\n",
    "\n",
    "  # TODO: Implement the training logic for the two agents\n",
    "\n",
    "  return td_agent, mc_agent, td_values, mc_values, td_msve, mc_msve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2mLF6KDp5f3"
   },
   "outputs": [],
   "source": [
    "# TODO: create the two versions of the LinearChainEnv\n",
    "\n",
    "# TODO: For each env, train your agents and use the below code to plot your values and calculate mean MSVE\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(td_values, label=\"TD(0)\")\n",
    "# plt.plot(mc_values, label=\"MC\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"V(start state)\")\n",
    "# plt.title(\"Start state value over episodes\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(td_msve, label=\"TD(0)\")\n",
    "# plt.plot(mc_msve, label=\"MC\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"MSVE\")\n",
    "# plt.title(\"Mean Squared Value Error over episodes\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# mean_td_msve = sum(td_msve) / len(td_msve)\n",
    "# mean_mc_msve = sum(mc_msve) / len(mc_msve)\n",
    "\n",
    "# print(\"Mean MSVE TD(0):\", mean_td_msve)\n",
    "# print(\"Mean MSVE MC:\", mean_mc_msve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHTzAZGawv7E"
   },
   "source": [
    "### **Explain your choice of parameter value and how it affects performance in TD0 and MC:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLpSxQBCw35P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
